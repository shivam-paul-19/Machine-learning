{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265ea7f1",
   "metadata": {},
   "source": [
    "## **Tokenization**\n",
    "Tokenization is the process of replacing sensitive data (like credit card numbers) with unique, non-sensitive placeholders called tokens, or breaking down text into smaller units (words/subwords) for NLP. It enhances data security by removing PII from systems and enables efficient AI language processing. \n",
    "\n",
    "### Terminoligies\n",
    "- **Corpus**: A whole Paragraph\n",
    "- **Document**: A sentence\n",
    "- **Vocabulary**: All the *Unique* words\n",
    "- **Words**: All the words (total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd283e",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e6aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza originated in Naples in the 18th century — as a simple street food for the poor — topped with tomatoes, cheese, and herbs! \n",
      "In 1889, baker Raffaele Esposito created \"Pizza Margherita,\" honoring Queen Margherita of Savoy... iconic, right?\n",
      "Italian immigrants later popularized pizza in the United States; making it world's favourite dish.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"Pizza originated in Naples in the 18th century — as a simple street food for the poor — topped with tomatoes, cheese, and herbs! \n",
    "In 1889, baker Raffaele Esposito created \"Pizza Margherita,\" honoring Queen Margherita of Savoy... iconic, right?\n",
    "Italian immigrants later popularized pizza in the United States; making it world's favourite dish.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb27be6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pizza originated in Naples in the 18th century — as a simple street food for the poor — topped with tomatoes, cheese, and herbs!',\n",
       " 'In 1889, baker Raffaele Esposito created \"Pizza Margherita,\" honoring Queen Margherita of Savoy... iconic, right?',\n",
       " \"Italian immigrants later popularized pizza in the United States; making it world's favourite dish.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "\n",
    "# breaking into sentence\n",
    "documents = sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486c0f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pizza', 'originated', 'in', 'Naples', 'in', 'the', '18th', 'century', '—', 'as', 'a', 'simple', 'street', 'food', 'for', 'the', 'poor', '—', 'topped', 'with', 'tomatoes', ',', 'cheese', ',', 'and', 'herbs', '!']\n",
      "['In', '1889', ',', 'baker', 'Raffaele', 'Esposito', 'created', '``', 'Pizza', 'Margherita', ',', \"''\", 'honoring', 'Queen', 'Margherita', 'of', 'Savoy', '...', 'iconic', ',', 'right', '?']\n",
      "['Italian', 'immigrants', 'later', 'popularized', 'pizza', 'in', 'the', 'United', 'States', ';', 'making', 'it', 'world', \"'s\", 'favourite', 'dish', '.']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ecabae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pizza', 'originated', 'in', 'Naples', 'in', 'the', '18th', 'century', '—', 'as', 'a', 'simple', 'street', 'food', 'for', 'the', 'poor', '—', 'topped', 'with', 'tomatoes', ',', 'cheese', ',', 'and', 'herbs', '!']\n",
      "['In', '1889', ',', 'baker', 'Raffaele', 'Esposito', 'created', '\"', 'Pizza', 'Margherita', ',\"', 'honoring', 'Queen', 'Margherita', 'of', 'Savoy', '...', 'iconic', ',', 'right', '?']\n",
      "['Italian', 'immigrants', 'later', 'popularized', 'pizza', 'in', 'the', 'United', 'States', ';', 'making', 'it', 'world', \"'\", 's', 'favourite', 'dish', '.']\n"
     ]
    }
   ],
   "source": [
    "# treats punctunation as separate\n",
    "\n",
    "for doc in documents:\n",
    "    print(wordpunct_tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395b43e",
   "metadata": {},
   "source": [
    "## **Stemming**\n",
    "Stemming in NLP is a text preprocessing technique that reduces words to their base or root form (e.g., \"running\", \"runs\", \"runner\" become \"run\") by removing suffixes and prefixes. It is used for normalization in search engines, chatbots, and text mining to improve efficiency by reducing vocabulary size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f82b09",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a839685",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['running', 'runs', 'eating', 'walking', 'eaten', 'walks', 'goes', 'go', 'going', 'fairly', 'sportingly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abcfc06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: run\n",
      "runs: run\n",
      "eating: eat\n",
      "walking: walk\n",
      "eaten: eaten\n",
      "walks: walk\n",
      "goes: goe\n",
      "go: go\n",
      "going: go\n",
      "fairly: fairli\n",
      "sportingly: sportingli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w}: {porter.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a2dce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: run\n",
      "runs: run\n",
      "eating: eat\n",
      "walking: walk\n",
      "eaten: eaten\n",
      "walks: walk\n",
      "goes: goe\n",
      "go: go\n",
      "going: go\n",
      "fairly: fair\n",
      "sportingly: sport\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow = SnowballStemmer(language='english')\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w}: {snow.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb9e89d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: runn\n",
      "runs: runs\n",
      "eating: eat\n",
      "walking: walk\n",
      "eaten: eat\n",
      "walks: walks\n",
      "goes: go\n",
      "go: go\n",
      "going: go\n",
      "fairly: fair\n",
      "sportingly: sporting\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "reg = RegexpStemmer(regexp=\"ing$|es$|ly$|$s|en$\")\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w}: {reg.stem(w)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
