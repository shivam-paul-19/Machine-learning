{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49f2886",
   "metadata": {},
   "source": [
    "# **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ea7f1",
   "metadata": {},
   "source": [
    "## **Tokenization**\n",
    "Tokenization is the process of replacing sensitive data (like credit card numbers) with unique, non-sensitive placeholders called tokens, or breaking down text into smaller units (words/subwords) for NLP. It enhances data security by removing PII from systems and enables efficient AI language processing. \n",
    "\n",
    "### Terminoligies\n",
    "- **Corpus**: A whole Paragraph\n",
    "- **Document**: A sentence\n",
    "- **Vocabulary**: All the *Unique* words\n",
    "- **Words**: All the words (total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd283e",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87e6aa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pizza is one of the world’s most beloved foods, known for its comforting flavors and endless variety. From simple street slices to gourmet creations, it brings people together across cultures and continents.\n",
      "Pizza originated in Naples, where flatbreads topped with tomatoes and cheese became popular among working-class families in the 18th and 19th centuries. The famous Margherita pizza, made with tomatoes, mozzarella, and basil, was created in honor of Queen Margherita of Savoy. Italian immigrants later introduced pizza to the United States, especially in New York City.\n",
      "Today, pizza is a global phenomenon, adapted to local tastes and enjoyed in countless creative forms.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "Pizza is one of the world’s most beloved foods, known for its comforting flavors and endless variety. From simple street slices to gourmet creations, it brings people together across cultures and continents.\n",
    "Pizza originated in Naples, where flatbreads topped with tomatoes and cheese became popular among working-class families in the 18th and 19th centuries. The famous Margherita pizza, made with tomatoes, mozzarella, and basil, was created in honor of Queen Margherita of Savoy. Italian immigrants later introduced pizza to the United States, especially in New York City.\n",
    "Today, pizza is a global phenomenon, adapted to local tastes and enjoyed in countless creative forms.\n",
    "\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb27be6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nPizza is one of the world’s most beloved foods, known for its comforting flavors and endless variety.',\n",
       " 'From simple street slices to gourmet creations, it brings people together across cultures and continents.',\n",
       " 'Pizza originated in Naples, where flatbreads topped with tomatoes and cheese became popular among working-class families in the 18th and 19th centuries.',\n",
       " 'The famous Margherita pizza, made with tomatoes, mozzarella, and basil, was created in honor of Queen Margherita of Savoy.',\n",
       " 'Italian immigrants later introduced pizza to the United States, especially in New York City.',\n",
       " 'Today, pizza is a global phenomenon, adapted to local tastes and enjoyed in countless creative forms.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "\n",
    "# breaking into sentence\n",
    "documents = sent_tokenize(corpus)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "486c0f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pizza', 'is', 'one', 'of', 'the', 'world', '’', 's', 'most', 'beloved', 'foods', ',', 'known', 'for', 'its', 'comforting', 'flavors', 'and', 'endless', 'variety', '.']\n",
      "['From', 'simple', 'street', 'slices', 'to', 'gourmet', 'creations', ',', 'it', 'brings', 'people', 'together', 'across', 'cultures', 'and', 'continents', '.']\n",
      "['Pizza', 'originated', 'in', 'Naples', ',', 'where', 'flatbreads', 'topped', 'with', 'tomatoes', 'and', 'cheese', 'became', 'popular', 'among', 'working-class', 'families', 'in', 'the', '18th', 'and', '19th', 'centuries', '.']\n",
      "['The', 'famous', 'Margherita', 'pizza', ',', 'made', 'with', 'tomatoes', ',', 'mozzarella', ',', 'and', 'basil', ',', 'was', 'created', 'in', 'honor', 'of', 'Queen', 'Margherita', 'of', 'Savoy', '.']\n",
      "['Italian', 'immigrants', 'later', 'introduced', 'pizza', 'to', 'the', 'United', 'States', ',', 'especially', 'in', 'New', 'York', 'City', '.']\n",
      "['Today', ',', 'pizza', 'is', 'a', 'global', 'phenomenon', ',', 'adapted', 'to', 'local', 'tastes', 'and', 'enjoyed', 'in', 'countless', 'creative', 'forms', '.']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01ecabae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pizza', 'is', 'one', 'of', 'the', 'world', '’', 's', 'most', 'beloved', 'foods', ',', 'known', 'for', 'its', 'comforting', 'flavors', 'and', 'endless', 'variety', '.']\n",
      "['From', 'simple', 'street', 'slices', 'to', 'gourmet', 'creations', ',', 'it', 'brings', 'people', 'together', 'across', 'cultures', 'and', 'continents', '.']\n",
      "['Pizza', 'originated', 'in', 'Naples', ',', 'where', 'flatbreads', 'topped', 'with', 'tomatoes', 'and', 'cheese', 'became', 'popular', 'among', 'working', '-', 'class', 'families', 'in', 'the', '18th', 'and', '19th', 'centuries', '.']\n",
      "['The', 'famous', 'Margherita', 'pizza', ',', 'made', 'with', 'tomatoes', ',', 'mozzarella', ',', 'and', 'basil', ',', 'was', 'created', 'in', 'honor', 'of', 'Queen', 'Margherita', 'of', 'Savoy', '.']\n",
      "['Italian', 'immigrants', 'later', 'introduced', 'pizza', 'to', 'the', 'United', 'States', ',', 'especially', 'in', 'New', 'York', 'City', '.']\n",
      "['Today', ',', 'pizza', 'is', 'a', 'global', 'phenomenon', ',', 'adapted', 'to', 'local', 'tastes', 'and', 'enjoyed', 'in', 'countless', 'creative', 'forms', '.']\n"
     ]
    }
   ],
   "source": [
    "# treats punctunation as separate\n",
    "\n",
    "for doc in documents:\n",
    "    print(wordpunct_tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0395b43e",
   "metadata": {},
   "source": [
    "## **Stemming**\n",
    "Stemming in NLP is a text preprocessing technique that reduces words to their base or root form (e.g., \"running\", \"runs\", \"runner\" become \"run\") by removing suffixes and prefixes. It is used for normalization in search engines, chatbots, and text mining to improve efficiency by reducing vocabulary size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f82b09",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a839685",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['running', 'runs', 'eating', 'walking', 'eaten', 'walks', 'goes', 'go', 'going', 'fairly', 'sportingly']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04ae02",
   "metadata": {},
   "source": [
    "#### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "abcfc06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: run\n",
      "runs: run\n",
      "eating: eat\n",
      "walking: walk\n",
      "eaten: eaten\n",
      "walks: walk\n",
      "goes: goe\n",
      "go: go\n",
      "going: go\n",
      "fairly: fairli\n",
      "sportingly: sportingli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w}: {porter.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a364ff53",
   "metadata": {},
   "source": [
    "#### Snowball stemmer\n",
    "Porter stemmer works for most of the words but we can see it is not working properly for *eaten*, *fairly* and *sportingly*. That's why `SnowballStemmer` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a2dce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: run\n",
      "runs: run\n",
      "eating: eat\n",
      "walking: walk\n",
      "eaten: eaten\n",
      "walks: walk\n",
      "goes: goe\n",
      "go: go\n",
      "going: go\n",
      "fairly: fair\n",
      "sportingly: sport\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow = SnowballStemmer(language='english')\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w}: {snow.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f99b4",
   "metadata": {},
   "source": [
    "#### RegexpStemmer\n",
    "It allows the programmer to decide the suffix, which gives more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb9e89d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running: runn\n",
      "runs: runs\n",
      "eating: eat\n",
      "walking: walk\n",
      "eaten: eat\n",
      "walks: walks\n",
      "goes: go\n",
      "go: go\n",
      "going: go\n",
      "fairly: fair\n",
      "sportingly: sporting\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "reg = RegexpStemmer(regexp=\"ing$|es$|ly$|$s|en$\")\n",
    "\n",
    "for w in words:\n",
    "    print(f\"{w}: {reg.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fd2a7",
   "metadata": {},
   "source": [
    "We can see this performs well in *eaten* also as we have defined \"en$\" in regex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c150842",
   "metadata": {},
   "source": [
    "## **Lemmatization**\n",
    "Lemmatization in NLP is a text normalization technique that reduces inflected or derived words to their base dictionary form, known as a lemma (e.g., \"running\" to \"run,\" \"better\" to \"good\"). Unlike stemming, it utilizes vocabulary, morphological analysis, and part-of-speech (POS) tagging to ensure accuracy. It is crucial for improving search relevance, chatbot context, and reducing data redundancy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b931d4a",
   "metadata": {},
   "source": [
    "### Impelmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8aaea869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "682ecfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic use case\n",
    "lem.lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983540f",
   "metadata": {},
   "source": [
    "> \"running\" came as \"running\"? why?\n",
    "\n",
    "Because by defualt, the wordNetLemmatizer assumes every word as 'Noun'. We have to define the `pos` parameter accordingly\n",
    "\n",
    "- noun -> n\n",
    "- adjective -> a\n",
    "- adverb -> r\n",
    "- verb -> v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f91a041f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"running\", pos='v')   # now output will come as 'run'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5377c3",
   "metadata": {},
   "source": [
    "However it is not prossible to hardcode every pos tag so for this puprose, spacy is better choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "997dbd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running : run (VERB)\n",
      "runs : run (NOUN)\n",
      "eating : eat (VERB)\n",
      "walking : walk (VERB)\n",
      "eaten : eat (VERB)\n",
      "walks : walk (NOUN)\n",
      "goes : go (VERB)\n",
      "go : go (VERB)\n",
      "going : go (VERB)\n",
      "fairly : fairly (ADV)\n",
      "sportingly : sportingly (ADV)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\" \".join(words))\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} : {token.lemma_} ({token.pos_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b275b99",
   "metadata": {},
   "source": [
    "## **Handling Stopwords**\n",
    "Stop words in NLP are common, high-frequency words (e.g., \"the,\" \"is,\" \"and,\" \"in\") filtered out during text preprocessing to reduce noise and improve model efficiency. They carry little semantic meaning, so removing them helps algorithms focus on significant words for tasks like search indexing, sentiment analysis, and topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f98fc968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "# we need to filter out these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1543d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwatch: ['the', 'of', 'the', 'was', 'on', 'the', 'and', 'about', 'the', 'that', 'had', 'not']\n",
      "Useful words: ['In', 'middle', 'day,', 'I', 'sitting', 'porch', 'thinking', 'things', 'I', 'yet', 'done.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In the middle of the day, I was sitting on the porch and thinking about the things that I had not yet done.\"\n",
    "\n",
    "stop = []\n",
    "useful_words = []\n",
    "\n",
    "for word in sentence.split(\" \"):\n",
    "    if word in stopwords.words('english'):\n",
    "        stop.append(word)\n",
    "    else:\n",
    "        useful_words.append(word)\n",
    "\n",
    "print(f\"Stopwatch: {stop}\")\n",
    "print(f\"Useful words: {useful_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9de39",
   "metadata": {},
   "source": [
    "## Creating a whole pipleline for text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e43a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class textPreprocessor:\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "    \n",
    "    def process(self) -> list:\n",
    "        # tokenise\n",
    "        words = word_tokenize(self.corpus)\n",
    "\n",
    "        # remove stopwords\n",
    "        words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "        # find root word\n",
    "        stem = spacy.load(\"en_core_web_sm\")\n",
    "        doc = stem(\" \".join(words))\n",
    "\n",
    "        # final list\n",
    "        final_list = []\n",
    "        for token in doc:\n",
    "            final_list.append(token.lemma_)\n",
    "        \n",
    "        return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dd027a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = textPreprocessor(corpus=corpus)\n",
    "list_of_words = pre.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67a0738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pizza one world ' beloved food , know comfort flavor endless variety . from simple street slice gourmet creation , bring people together across culture continent . Pizza originate Naples , flatbread top tomato cheese become popular among working - class family 18th 19th century . the famous Margherita pizza , make tomato , mozzarella , basil , create honor Queen Margherita Savoy . italian immigrant later introduce pizza United States , especially New York City . today , pizza global phenomenon , adapt local taste enjoy countless creative form .\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(list_of_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
