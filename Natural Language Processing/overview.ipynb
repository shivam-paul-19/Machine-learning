{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123cceaa",
   "metadata": {},
   "source": [
    "# **Natural Language Processing**\n",
    "Natural Language Processing (NLP) is a branch of Artificial Intelligence that enables computers to understand, interpret, analyze, and generate human language. It combines linguistics, computer science, and machine learning to process textual or spoken data in a meaningful way.\n",
    "\n",
    "## Why NLP is Needed\n",
    "\n",
    "- Humans communicate in natural language, not binary or structured formats\n",
    "- Massive amount of unstructured text data exists (social media, emails, reviews, chats)\n",
    "- Enables automation of language tasks (chatbots, translation, sentiment analysis)\n",
    "- Helps extract useful insights from text data\n",
    "- Bridges communication gap between humans and machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bbbf80",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "Text preprocessing is the initial step in NLP where raw text data is cleaned and transformed into a structured format suitable for analysis. Since natural language is unstructured and noisy, preprocessing improves model performance and accuracy.\n",
    "\n",
    "- #### **Tokenization**:\n",
    "    Tokenization is the process of breaking text into smaller units called tokens.\n",
    "    These tokens can be:\n",
    "\n",
    "    - Words\n",
    "    - Sentences\n",
    "    - Subwords\n",
    "\n",
    "    Example:\n",
    "    ‚ÄúMachine learning is amazing‚Äù ‚Üí\n",
    "    [Machine, learning, is, amazing]\n",
    "\n",
    "    It converts continuous text into manageable pieces.\n",
    "\n",
    "- #### **Stemming**:\n",
    "    Stemming reduces words to their root form by removing suffixes.\n",
    "    The resulting word may not always be grammatically correct.\n",
    "\n",
    "    Examples:\n",
    "    - Playing ‚Üí Play\n",
    "    - Studies ‚Üí Studi\n",
    "    - Running ‚Üí Run\n",
    "\n",
    "    It is rule-based and faster, but less accurate.\n",
    "\n",
    "- #### **Lemmatization**:\n",
    "\n",
    "    Lemmatization reduces words to their base or dictionary form (lemma) using linguistic knowledge.\n",
    "\n",
    "    Examples:\n",
    "    - Running ‚Üí Run\n",
    "    - Better ‚Üí Good\n",
    "    - Studies ‚Üí Study\n",
    "\n",
    "    Unlike stemming, the output is a valid word.\n",
    "    It is more accurate but computationally heavier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ff7b0",
   "metadata": {},
   "source": [
    "## Text Vectorization\n",
    "Text vectorization is the process of converting textual data into numerical representations so that machine learning models can process and analyze it.\n",
    "Since algorithms operate on numbers, vectorization transforms words, sentences, or documents into mathematical vectors.\n",
    "\n",
    "- #### **Bag of Words (BoW)**:\n",
    "    Bag of Words represents text by counting the frequency of words in a document, ignoring grammar and word order.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - Create a vocabulary of all unique words\n",
    "    - Count how many times each word appears in a document\n",
    "    - Represent document as a frequency vector\n",
    "\n",
    "    Example:\n",
    "\n",
    "    Sentence 1: ‚ÄúI love NLP‚Äù\n",
    "    Sentence 2: ‚ÄúI love ML‚Äù\n",
    "\n",
    "    Vocabulary ‚Üí [I, love, NLP, ML]\n",
    "\n",
    "    Vectors:\n",
    "    - S1 ‚Üí [1, 1, 1, 0]\n",
    "    - S2 ‚Üí [1, 1, 0, 1]\n",
    "\n",
    "- #### **TF-IDF (Term Frequency ‚Äì Inverse Document Frequency)**:\n",
    "    TF-IDF improves BoW by giving importance to meaningful words and reducing the weight of common words.\n",
    "\n",
    "    Components:\n",
    "\n",
    "    - TF (Term Frequency) ‚Üí How often a word appears in a document\n",
    "    - IDF (Inverse Document Frequency) ‚Üí Penalizes words that appear in many documents\n",
    "\n",
    "    Formula idea:\n",
    "    TF-IDF = TF √ó IDF\n",
    "\n",
    "    Words like ‚Äúthe‚Äù, ‚Äúis‚Äù get low weight.\n",
    "    Rare but important words get high weight.\n",
    "\n",
    "- #### **N-grams**:\n",
    "    N-grams consider sequences of N consecutive words instead of single words.\n",
    "\n",
    "    Types:\n",
    "    - Unigram ‚Üí single word\n",
    "    - Bigram ‚Üí two words\n",
    "    - Trigram ‚Üí three words\n",
    "\n",
    "    Example:\n",
    "\n",
    "    Sentence: ‚ÄúI love NLP‚Äù\n",
    "\n",
    "    Unigrams ‚Üí I, love, NLP\n",
    "    Bigrams ‚Üí I love, love NLP\n",
    "\n",
    "    Why needed?\n",
    "\n",
    "    Because ‚Äúnot good‚Äù ‚â† ‚Äúgood‚Äù\n",
    "\n",
    "    BoW misses that.\n",
    "    N-grams partially capture context.\n",
    "\n",
    "- #### **Word2Vec**:\n",
    "    Word2Vec is a neural network-based model that converts words into dense vectors based on context.\n",
    "\n",
    "    Core idea:\n",
    "    Words appearing in similar contexts have similar meanings.\n",
    "\n",
    "    Example:\n",
    "    King ‚Äì Man + Woman ‚âà Queen\n",
    "    (Yes, algebra with words üòè)\n",
    "\n",
    "    Features:\n",
    "\n",
    "    - Dense vectors (low dimensional, like 100‚Äì300 dims)\n",
    "    - Captures semantic similarity\n",
    "    - Context-aware (to some extent)\n",
    "\n",
    "- #### **Average Word2Vec**:\n",
    "    Word2Vec gives vectors for individual words.\n",
    "    But what about a sentence?\n",
    "\n",
    "    Solution:\n",
    "    Take average of all word vectors in the sentence.\n",
    "\n",
    "    Example:\n",
    "    Sentence = [word1_vec + word2_vec + word3_vec] / 3"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
