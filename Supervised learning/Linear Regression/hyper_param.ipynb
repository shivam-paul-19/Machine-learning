{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100e3d73",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cf29d",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "Ridge Regression is a machine learning technique that enhances linear regression by adding an L2 penalty to the loss function, shrinking coefficients to prevent overfitting, especially with many correlated features (multicollinearity). It introduces bias to reduce variance, creating more stable, generalized models that perform better on new data by penalizing large coefficient values, unlike standard regression that can become unstable. \n",
    "\n",
    "![ridge](https://www.appliedaicourse.com/blog/wp-content/uploads/2024/10/ridge-regression-in-machine-learning.webp)\n",
    "\n",
    "### The problem of overfitting\n",
    "overfitting is a condition where a model performs too well on the test data set, this is actually not a desirable situation as this can the model to be highly baised towards the test data set and it is high chance that the model won't perform well on the **train data set** or **new points**\n",
    "\n",
    "The **Ridge** penalises the coefficients to prevent overfitting.\n",
    "\n",
    "#### **How it works**\n",
    "it adds a **λ** and **slope²** in the cost function (here MSE is taken as example), **λ** is a *hyperparameter*, i.e., adjusted by the programmer\n",
    "\n",
    "$$ C(θ₀, θ₁) = \\sum^{n}_{i=1} \\frac{(y-ŷ)²}{n} + λ \\sum^{p}_{j=1} (slopeⱼ)² $$\n",
    "$$ n = number\\ of\\ data\\ points $$\n",
    "$$ p = number\\ of\\ features $$\n",
    "\n",
    "note: the *λ* and *slope* are inversely proportional, as we increase the λ, the slope value decreases (but never gets 0)\n",
    "\n",
    "![gradient](https://miro.medium.com/v2/resize:fit:1400/1*m1vR4evYBV7NHZUJLvpH5A.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c73fa3",
   "metadata": {},
   "source": [
    "## Lasso regression\n",
    "Lasso regression is a method that performs both variable selection and regularization by adding a penalty term to the standard linear regression (Ordinary Least Squares) objective function. It is specifically known for its use of the L1 norm as a penalty, which enables the model to produce \"sparse\" solutions—meaning some coefficients are reduced to exactly zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e507e9",
   "metadata": {},
   "source": [
    "### The need of feature selection\n",
    "In the context of Lasso regression, feature selection is the process of identifying a small subset of the most relevant variables while discarding the rest by shrinking their coefficients to exactly zero.\n",
    "\n",
    "#### **How it works**\n",
    "it adds a **λ** and **|slope|** in the cost function (here MSE is taken as example), **λ** is a *hyperparameter*, i.e., adjusted by the programmer\n",
    "\n",
    "$$ C(θ₀, θ₁) = \\sum^{n}_{i=1} \\frac{(y-ŷ)²}{n} + λ \\sum^{p}_{j=1} |slopeⱼ| $$\n",
    "$$ n = number\\ of\\ data\\ points $$\n",
    "$$ p = number\\ of\\ features $$\n",
    "\n",
    "Here also, the *λ* and *slope* are inversely related, and here the value can become 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f6568",
   "metadata": {},
   "source": [
    "## ElasticNet\n",
    "Elastic Net is a hybrid regularization technique that combines both Ridge (L2) and Lasso (L1) penalties into a single cost function. It was created to overcome the individual limitations of each method.\n",
    "\n",
    "$$ C(θ₀, θ₁) = \\sum^{n}_{i=1} \\frac{(y-ŷ)²}{n} + λ₁ \\sum^{p}_{j=1} (slopeⱼ)² + λ₂ \\sum^{p}_{j=1} |slopeⱼ| $$\n",
    "$$ n = number\\ of\\ data\\ points $$\n",
    "$$ p = number\\ of\\ features $$\n",
    "\n",
    "It is used to levarage both the things, it solves the problem of ***overfitting*** and ***feature selection***"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
